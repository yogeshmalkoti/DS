#download data

library(Rborist)
library(ggpubr)
library(caret)
library(tidyverse)


carfile<-"http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data"

#if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
#if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

dl <- tempfile()
download.file(carfile, dl)
cars <- str_split_fixed(readLines(dl), ",", 7)
colnames(cars) <- c("buying", "maint", "door", "persons", "lug_boot", "safety", "decision")

summary(cars)

rm(dl)


car_frame<-as.data.frame(cars)

## lets create train and test data set out of it.

#y<-car_frame$decision

car_matrix<- createDataPartition(y=car_frame$decision, times = 1, p = 0.5, list = FALSE)

train<-car_frame[-car_matrix,]
validation<-car_frame[car_matrix,]

# Step 2) Exploratory Data Analysis
# This sections helps to understand the structure of the car dataset 
# in order to use the insights for a better prediction of movie ratings.

# lets plot a graph for count vs each decision class

#car_matrix %>% ggplot(decision)

qplot(car_frame$decision, col = I("red"))

table(car_frame$decision)
# As we can see from this plot that data is skewed towards the unacc class as it has 
#  more outcomes for unacc class.

# now lets plot the effect of each predictor on the decision.

plot_person<-car_frame %>% ggplot(aes(x = persons, fill = decision)) + geom_bar() + ggtitle("Seating capacity VS Decision") 
plot_buying<-car_frame %>% ggplot(aes(x = buying, fill = decision)) + geom_bar() + ggtitle("Buying Cost VS Decision") 
plot_maint<-car_frame %>% ggplot(aes(x = maint, fill = decision)) + geom_bar() + ggtitle("Maintenance Cost VS Decision") 
plot_logboot<-car_frame %>% ggplot(aes(x = lug_boot, fill = decision)) + geom_bar() + ggtitle("BootSpace VS Decision") 
plot_safety<-car_frame %>% ggplot(aes(x = safety, fill = decision)) + geom_bar() + ggtitle("Safety VS Decision") 
plot_door<-car_frame %>% ggplot(aes(x = door, fill = decision)) + geom_bar() + ggtitle("No of Doors  VS Decision") 



ggarrange(plot_person, plot_buying, plot_maint, plot_logboot, plot_safety, plot_door, widths = 1:1)


# Lets use various regression method and enseble the best prediction.

models <- c("knn", "rf", "ranger",  
            "wsrf", "Rborist", 
            "svmRadial", "svmRadialCost", "svmRadialSigma")
 
 
fits <- lapply(models, function(model){ 
   print(model)
  train(decision ~ ., method = model, data = train)
})


names(fits) <- models

##Now we have created a list of fitted models with different algorithms as specified in 
## model selection. and we can acces the fitted model as 

#fits[1] , fits[2].. etc

# now we will use this array of fitted model and test with our mnist_27$test data set to get the 
# predictions generated by different models.


decision_hat_matrix<- sapply(fits, function(fit){
  print(fit)
  predict(fit, train)
})


###
## so we have generated a output of 200 * 3 with each coloumn contains the model type and ech coloumn conains the respective predictor for corrosponding the outcome.

## now lets calcluate the  prediction of all the models with the original data set and get the mean of all the prediction.
## please note the functions used. we can use ConfusionMatrix to get the accuracy but mean seams easy
## to use at this example.


accuracy_by_models <- colMeans(decision_hat_matrix == train$decision)
accuracy_by_models
mean(accuracy_by_models)


### now lets build the enseble prediction by majority vote and compute the accuracy 
## of the ensemble.

decision_pred_majority<- apply(decision_hat_matrix, 1, function(x){
  tail(names(sort(table(x))), 1)
})


## Now lets calculate the mean of ensebled prediction ###

ensemb_mean<-mean(decision_pred_majority == train$decision)

# 0.98 

#Same can be confirmed from confusion matrix
# now lets calculate confusionmatrix with ensembled predicted values to the original.

confusionMatrix(factor(decision_pred_majority), train$decision)


# 0.9896


## lets find which models performed better than ensebled ones.


ind <- accuracy_by_models > mean(decision_pred_majority == train$decision)
sum(ind)
models[ind]

# From here we came to know the two models which are performing well are below:

#[1] "rf"     "ranger"

# Lets remove the models which are performing less than mean of training accuracy
# and generate the ensebles predictions and generate/observe accuracy .


models <- c( "rf", "ranger")

fits <- lapply(models, function(model){ 
  print(model)
  train(decision ~ ., method = model, data = train)
})


names(fits) <- models


decision_hat_matrix<- sapply(fits, function(fit){
  print(fit)
  predict(fit, train)
})

accuracy_by_models <- colMeans(decision_hat_matrix == train$decision)
accuracy_by_models
mean(accuracy_by_models)


decision_pred_majority<- apply(decision_hat_matrix, 1, function(x){
  tail(names(sort(table(x))), 1)
})


ensemb_mean<-mean(decision_pred_majority == train$decision)

confusionMatrix(factor(decision_pred_majority), train$decision)


### Lets use rf as its accuracy is almost 100 on set.
### lets train our model with it and try on validation test and verify accuracy.

rtrain<-train(decision ~ ., method = "rf", data = train)

## lets try on out validation set.

decision_predict<- predict(rtrain, validation)

confusionMatrix(decision_predict, validation$decision)

#0.9168

rangertrain<-train(decision ~ ., method = "ranger", data = train)

## lets try on out validation set.

decision_predict<- predict(rangertrain, validation)

confusionMatrix(decision_predict, validation$decision)

#0.9145
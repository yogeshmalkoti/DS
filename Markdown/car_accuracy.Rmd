---
title: "Predict Car Acceptability"
author: "Yogesh Kumar Malkoti"
date: "May 24, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This report is part of the capstone project of the EdX course ‘HarvardX: PH125.9x Data Science: Capstone - Choose Your own project’. Its goal is to demonstrate that the student acquired skills with the R programming language in the field of datascience to actually solve realworld problems.
The task is to analyse a Car Evaluation Data Set 
called ‘cars’ which contains 1728 data about car's criteria. The insights from this analysis are used to generate & Predict Car Acceptability which are compared with the actual ratings to check the quality of the prediction algorithm.


# Summary
The report is split in three sections. First, the dataset is loaded and preformated for further analysis. Second, an exploratory datan alysis helps to understand the structure of the dataset. Finally, a machine learning algorithm creates predictions which are then exported for a final test. We haev four class of outcomes available which we have to predict from 6 predictors. A ‘Random Forest’ approach was used a final model after ensembling through variuos regression algorithms. This algorithm achieved an accuracy of 91% with decent Sensitivity and Specificity across the classes.

## Data Set Information:

Car Evaluation Database was derived from a simple hierarchical decision model originally developed for the demonstration of DEX, M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.). The model evaluates cars according to the following concept structure: 

### Car Acceptability Factors.
- PRICE: overall price  
   buying: buying price  
   maint: price of the maintenance   
- TECH technical characteristics  
   COMFORT: comfort  
   doors: number of doors   
   persons: capacity in terms of persons to carry   
   lug_boot: the size of luggage boot  
   Safety: estimated safety of the car  

Input attributes are printed in lowercase. Besides the target concept (CAR), the model includes three intermediate concepts: PRICE, TECH, COMFORT. Every concept is in the original model related to its lower level descendants by a set of examples.

The Car Evaluation Database contains examples with the structural information removed, i.e., directly relates CAR to the six input attributes: buying, maint, doors, persons, lug_boot, safety. 

Because of known underlying concept structure, this database may be particularly useful for testing constructive induction and structure discovery methods. 

### Attribute Information:

### Class Values: 

unacc, acc, good, vgood 

### Attributes: 

buying: vhigh, high, med, low  
maint: vhigh, high, med, low.  
doors: 2, 3, 4, 5more.  
persons: 2, 4, more.  
lug_boot: small, med, big.   
safety: low, med, high.  


# Random Forest
### Method Description
In Random Forests the idea is to decorrelate the several trees which are generated by the different bootstrapped samples from training Data. And then we simply reduce the Variance in the Trees by averaging them.
Averaging the Trees helps us to reduce the variance and also improve the Performance of Decision Trees on Test Set and eventually avoid Overfitting.
The idea is to build lots of Trees in such a way to make the Correlation between the Trees smaller.The effect of using all predictors is that each tree uses different predictors to split data at various times. This means that 2 trees generated on same training data will have randomly different variables selected at each split, hence this is how the trees will get de-correlated and will be independent of each other. Another great thing about Random Forests and Bagging is that we can keep on adding more and more big bushy trees and that won’t hurt us because at the end we are just going to average them out which will reduce the variance by the factor of the number of Trees.




## Step 1) Download MovieLens Data
I am using below url for the cars data set.

http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data

The dataset ‘cars’ gets split into a training-testset called ‘train’ and a set for validation purposes called ‘validation’.


```{r Load the cars data}

carfile<-"http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data"

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(Rborist)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(ggpubr)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")


dl <- tempfile()
download.file(carfile, dl)
cars <- str_split_fixed(readLines(dl), ",", 7)
colnames(cars) <- c("buying", "maint", "door", "persons", "lug_boot", "safety", "decision")

summary(cars)

car_frame<-as.data.frame(cars)

## lets create train and test data set out of it.

#y<-car_frame$decision

car_matrix<- createDataPartition(y=car_frame$decision, times = 1, p = 0.5, list = FALSE)

train<-car_frame[-car_matrix,]
validation<-car_frame[car_matrix,]

rm(dl)
```

## Step 2) Exploratory Data Analysis
This sections helps to understand the structure of the cars dataset in order to use the insights for a better prediction of acceptance.

## Let's find number of rows and columns in the train dataset.

```{r Exploratory Data Analysis - Step 1}

paste('The Cars dataset has',nrow(train),'rows and',ncol(train),'columns.')

```

As we can see the train sample quit low so ML algorithms can be run on home run laptop/desktop.




```{r Exploratory Data Analysis - Step 2}

qplot(car_frame$decision, col = I("red"))

table(car_frame$decision)

```
We can see the data set contains most of the cceptability related to "unacc" class set.




```{r Exploratory Data Analysis - Step 3}

plot_person<-train %>% ggplot(aes(x = persons, fill = decision)) + geom_bar() + ggtitle("Seating capacity VS Decision") 
plot_buying<-train %>% ggplot(aes(x = buying, fill = decision)) + geom_bar() + ggtitle("Buying Cost VS Decision") 
plot_maint<-train %>% ggplot(aes(x = maint, fill = decision)) + geom_bar() + ggtitle("Maintenance Cost VS Decision") 
plot_logboot<-train %>% ggplot(aes(x = lug_boot, fill = decision)) + geom_bar() + ggtitle("BootSpace VS Decision") 
plot_safety<-train %>% ggplot(aes(x = safety, fill = decision)) + geom_bar() + ggtitle("Safety VS Decision") 
plot_door<-train %>% ggplot(aes(x = door, fill = decision)) + geom_bar() + ggtitle("No of Doors  VS Decision") 


ggarrange(plot_person, plot_buying, plot_maint, plot_logboot, plot_safety, plot_door, widths = 1:1)

```

From this plot we can see that Seating Capacity and Safety are the two major factors along with other predcitors while making a decision on car acceptance. Hence we need to get higher sensitivity over these two factors than others.


# Results
The challenge was to get the highest accuracy, along with higher sensitivity on two predictoes namely "Capacity" and "Safety". Here the outcome is a four class output with 6 predictors numeric as well as non numeric. Also from expolratory analysis of data we can see there is decision based approach of user while buying the vehicle depends on this factor. I tried a varietey of machine learning regression algorithms on the train data using ensebled approach and figured out Random Forest produng the best accuracy over train set of alost 99%. I then trained the RF model on train set and used on validation test with accuracy of 91% with very good sensitity.

## Train the model with Random forest algorithm.

```{r Build and train the model with RF algorithm on Train data }

rtrain<-train(decision ~ ., method = "rf", data = train)

```

```{r Get the predictions on train set  }

decision_predict<- predict(rtrain, train)

```

```{r Get the accuracy and confusion matrix with train set }

confusionMatrix(decision_predict, train$decision)

```

As we can see the accruacy is about % along with sesitivty for person and securoty predictors.

Now lets use this trained model on validation test.


```{r Get the predictions on validation set  }

decision_predict<- predict(rtrain, validation)

```

```{r Get the accuracy and confusion matrix with Validation set }

confusionMatrix(decision_predict, validation$decision)

```


# Export Predictions

```{r Exporting precitions along with originals to CSV}
# Ratings will go into the CSV submission file below:

write.csv(validation %>% mutate(predicted_decision = decision_predict),
          "car_predict.csv", na = "", row.names=FALSE)

```

# Conclusion
The aim of the project was to predict the acceptance of cars given a six factors from a data containing it. We accuracy is measured as absolute difference between the predicted value and the acutal value. We used supervised learning approach to train the model woth Random Forest algorithm and got a pretty high accuracy.



